It is data anlytics tools which is used for content categarisation, word analytics & sentiment analysis. 
In word analytics includes word count, word cloud, clustering, tagging
word count:
clustering: Classifies the text data into groups based on the word contained in the text.

In this project,I'm working on clustering module. I've done clustering in mahout. There are few steps involved in mahout
1) Convert input document into sequence file using java or this command bin/mahout seqdirectory \
        -i mahout-work/reuters-out \
        -o mahout-work/reuters-out-seqdir \
2) seq2sparse 
Generate tf-idf Vectors from Sequence files:

mahout seq2sparse \
   -i seqfiles/ \
   -o vectors/ \
   -ow \
   -x 90 \
  -ng 2 \
 -wt TF

-x 90 means that if a token appears in 90% of the documents it is considered a stop-word.
-ng The maximum size of ngrams to  create (2 = bigrams, 3 = trigrams, etc)
-wt The kind of weight to use. Currently TF or TFIDF. Default: TFIDF

The folder that’s produced using these command-line commands:
--->
	df-count/
	dictionary.file-0
	frequency.file-0
	tfidf-vectors/
	tf-vectors/
	tokenized-documents/
	wordcount/
Details:-
In the output folder, you’ll find a dictionary file and several directories. The dictionary file contains the mapping between a term and its integer ID. 
This file is useful when reading the output of different algorithms, so you need to retain it. The other folders are intermediate folders generated 
during the vectorization process, which happens in multiple steps (multiple MapReduce jobs).

 In the first step, the text documents are tokenized—they’re split into individual words using the Lucene StandardAnalyzer and stored in the tokenized-documents/
folder. The word-counting step—the n-gram generation step (which in this case only counts unigrams)—iterates through the tokenized documents and generates a set of
important words from the collection. The third step converts the tokenized documents into vectors using the term-frequency weight, thus creating TF vectors. By
default, the vectorizer uses the TF-IDF weighting, so two more steps happen after this:
the document-frequency (DF) counting job, and the TF-IDF vector creation. The TF-IDF weighted vectorized documents are found in the tfidf-vectors/ folder.
For most applications, you need just this folder and the dictionary file.

DF-count/ : this file contains word id and count across the document for example 
we have 10000 documents and word hadoop occurs in 1000 document so file will contain.
id count
10 1000
dictionary.file-0/ : The dictionary file contains the mapping between a term and its integer ID. 
06	0
2013	1
8	2
all	3
alliance	4
amazon	5
aws	6
azure	7
blog	8
boosts	9
business	10
an so on...

3) Kmeans

Generate  Kmean Clusters  from vectors:
 
mahout kmeans \
   -i vectors/tfidf-vectors/ \
   -c kmeans-centroids \
   -o kmeans-clusters \
   -k 20 \
   -ow \
   -dm org.apache.mahout.common.distance.CosineDistanceMeasure

-c - The input centroids, as Vectors.  Must be a SequenceFile of Writable, Cluster/Canopy
-k   The k in k-Means. If specified, then a random selection of k Vectors will   be chosen as the Centroid and written to the  clusters input   path.



Real word example of clustering 

Mahout Status.

	-Recommendatios
	-Clustering
		Clustering is all about organizing items from a given collection into groups of similar items or groups.
		Clustering a collection involves three things:
		 An algorithm—This is the method used to group the books together.
		 A notion of both similarity and dissimilarity—In the previous discussion, we relied
		on your assessment of which books belonged in an existing stack and which
		should start a new one.
		 A stopping condition—In the library example, this might be the point beyond
		which books can’t be stacked anymore, or when the stacks are already quite
		dissimilar. 
		Sweet - sugar, honey
		Shape - circle, square, triangle
		distance measures and their relationships with clustering. 
		methods that are popularly used for clustering.
		Suppose you were given the keys to a library containing thousands of books. The books were arranged in no particular
		order. Readers entering your library would have to sweep through all the books, one by one, to find a particular book. 
		Not only is this cumbersome and slow, it’s tedious as well. Sorting the books alphabetically by title would be a vast improvement—for readers
		searching for a book by title, that is. What if most people were simply browsing, or researching a general subject? Grouping the books by topic 
		would be more useful than an alphabetical-by-title ordering. But how would you even begin this grouping? Having just taken over this job, 
		you wouldn’t even be sure what all the books were about—surfing, romance, or topics you hadn’t encountered before. 
		-The strategy in the library examples was to merge stacks of books until some threshold was reached.
		The number of clusters formed in this example depended on the data; based on the number of books and the threshold, you might have ended up with 100, 20, or even
		just 1 cluster
		- measure of similarity - measure of similarity
		
		- Mahout contains various implementations of clustering, like k-means, fuzzy k-means, and canopy to name a few.
		- There are three steps involved in inputting data for the Mahout clustering algorithms: 
		   you need to preprocess the data, 
		   use that data to create vectors, 
		   and save the	vectors in SequenceFile format as input for the algorithm.
		- Mahout clustering algorithm reads sequence file as input.
		
		In Mahout, vectors are implemented as three different classes, each of which is optimized for different scenarios: 
		   DenseVector, RandomAccessSparseVector, and SequentialAccessSparseVector. 
		
		-- k-means clustering algorithm --
		
e.g 2)
 We have a great mental ability for finding repeating patterns, and
we continually associate what we see, hear, smell, and taste with things that are
already in our memory. For example, the taste of honey reminds us more of the
taste of sugar than salt. So we group together the things that taste like sugar and
honey and call them sweet. Without even knowing what sweet tastes like, we know
that all the sugary things in the world are similar and of the same group. We also
know how different they are from all the things belonging to the salty group.
Unconsciously, we group together tastes into such arms, and bigger heads. Apes look different from monkeys, but both are fond of
bananas. So we can think of apes and monkeys as two different groups or as a single
group of banana-loving primates.


Note: if the -k argument is supplied, any clusters in the -c directory will be overwritten and -k random points will be sampled from the input vectors to become the initial cluster centers.
In other words: if you know the initial cluster centers, supply them via -c and do not set -k. Otherwise an empty -c folder is okay, if you provide -k, the number of cluster centers to sample.


Commands to view results
mahout canopy -i path/tfidf-vectors -o path/canopy30  -ow -t1 600 -t2 300
mahout kmeans -i path/tfidf-vectors -o path/kmeans30 -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -c path/clusters-0-final -cd 0.5 -ow -x 20 -cl -k 30
mahout seqdumper path/clusters-20-final/part-r-00000 -o path\cluster1final.txt

mahout clusterdump --seqFileDir /data/clustering/kmeans/clusters-20-final --pointsDir path/clusteredPoints --output path\cluster1final.txt

mahout seq2sparse -i input -o out -ow <<<Other argument -seq true -nr 2 -ng 2 -a -wt tfidf>>>
 
 mahout clusterdump -dt sequencefile -d path/dictionary.file-0 -i path/clusters-7-final -o path\cluster7final.txt

 *** To view output of clustered points *** 
 mahout clusterdump --input path/clusters-20-final --pointsDir path/clusteredPoints --output path\clusteranalyze.txt
