
Q.1 What is the meaning of speculative execution in Hadoop? Why is it important?
Short Ans:-if task-tracker is not sending heartbeats to job-tracker then that particular task will be re-scheduled to another machine is called speculative execution.
by default, speculative execution in hadoop  can be disabled.
Ans:Hadoop uses this speculative execution to mitigate the slow-task problem. 
Hadoop does not wait for a task to get failed. Whenever it is seen that a task is running slow, 
the Hadoop platform will schedule redundant copies of that task across several nodes 
which do not have other work to perform. The parallel tasks are monitored. 
As soon as one of the parallel tasks finishes successfully, Hadoop will use its output and kill
the other parallel tasks. This process is termed as speculative execution. 

For enabling and disabling speculative execution of map and reduce tasks, 
the configuration parameters(Boolean) are mapred.map.tasks.speculative.execution 
and mapred.reduce.tasks.speculative.execution. By default both are set to true.

Q.2 Difference between hadoop fs -put and hadoop fs -copyFromLocal?
Ans1:Both are same intention just copy data from local system to HDFS, most often every Hadoop developer 
should aware of it.

Put:

Usage: hdfs dfs -put ...

Copy single src, or multiple srcs from local file system to the destination file system. 
Also reads input from stdin and writes to destination file system.

hdfs dfs -put localfile /user/hadoop/hadoopfile
hdfs dfs -put localfile1 localfile2 /user/hadoop/hadoopdir
hdfs dfs -put localfile hdfs://nn.example.com/hadoop/hadoopfile
hdfs dfs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.
CopyFromLocal:

Usage: hdfs dfs -copyFromLocal URI

Similar to put command, except that the source is restricted to a local file reference. 
hdfs dfs -copyFromLocal '/home/venu/Desktop/somethingfile.xml' hdfs://nn.example.com/hadoop/



Q.3 Hadoop input split size vs block size?

Block is the physical representation of data. Split is the logical representation of data present in Block.
Block and split size can be changed in properties.
Map reads data from Block through splits i.e. split act as a broker between Block and Mapper.
Consider two blocks:

Block 1
aa bb cc dd ee ff gg hh ii jj

Block 2
ww ee yy uu oo ii oo pp kk ll nn

Now map reads block 1 till aa to JJ and doesn't know how to read block 2 i.e. 
block doesn't know how to process different block of information. Here comes a Split it will form a 
Logical grouping of Block 1 and Block 2 as single Block, then it forms offset(key) and line (value) 
using inputformat and record reader and send map to process further processing.

If your resource is limited and you want to limit the number of maps you can increase the split size. 
For example: If we have 640 MB of 10 blocks i.e. each block of 64 MB and resource is limited then 
you can mention Split size as 128 MB then then logical grouping of 128 MB is formed and only 5 maps 
will be executed with a size of 128 MB.

If we specify split size is false then whole file will form one input split and processed by one map 
which it takes more time to process when file is big.

Q.4 How to find file biggest size in Unix /Hadoop
1) -> du -Sh | sort -rh | head -n 3 returns file size in human readable format in descending order

2) -> find . -type f -exec du -Sh {} + | sort -rh | head -n 5  

3) -> Finds directory 
        find . -type f -exec du -Sh {} + | sort -rh | head -n 5



