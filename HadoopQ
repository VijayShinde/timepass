Hadoop Questions
Qs. What is Hadoop framework? 
Hadoop is a free, Java¬based programming framework that supports the processing of large data sets in a distributed computing environment. Hadoop is part of the Apache project sponsored by the Apache Software Foundation. Hadoop makes it possible to run applications on systems with thousands of nodes involving thousands of terabytes. Its distributed file system facilitates rapid data transfer rates among nodes and allows the system to continue operating uninterrupted in case of a node failure. This approach lowers the risk of catastrophic system failure, even if a significant number of nodes become inoperative. Hadoop framework is used by major players including Google, Yahoo and IBM, largely for applications involving search engines and advertising. The preferred operating systems are Windows and Linux but Hadoop can also work with BSD and OS X 
Qs. What is MapReduce? 
MapReduce is a parallel programming model which is used to process large data sets across hundreds or thousands of servers in a Hadoop cluster. Map/reduce brings compute to the data at data location in contrast to traditional parallelism, which brings data to the compute location. The Term MapReduce is composed of Map and Reduce phase. The first is the map job, which takes a set of data and converts it into another set of data, where individual elements are broken down into tuples (key/value pairs). The reduce job takes the output from a map as input and combines those data tuples into a smaller set of tuples. As the sequence of the name MapReduce implies, the reduce job is always performed after the map job. he programming language for MapReduce is Java.All data emitted in the flow of a MapReduce program is in the form of Key/Value pairs. 
Qs. Explain a MapReduce program 
A MapReduce program consists of the following 3 parts : Driver Mapper Reducer 
1. The Driver code runs on the client machine and is responsible for building the configuration of the job and submitting it to the Hadoop Cluster. The Driver code will contain the main() method that accepts arguments from the command line. 
2. The Mapper code reads the input files as pairs and emits key value pairs. The Mapper class extends MapReduceBase and implements the Mapper interface. The Mapper interface expects four generics, which define the types of the input and output key/value pairs. The first two parameters define the input key and value types, the second two define the output key and value  types. 
3. The Reducer code reads the outputs generated by the different mappers as pairs and emits key value pairs. The Reducer class extends MapReduceBase and implements the Reducer interface. The Reducer interface expects four generics, which define the types of the input and output key/value pairs. The first two parameters define the intermediate key and value types, the second two define the final output key and value types. The keys are WritableComparables, the values are Writables 

Qs. Which interface needs to be implemented to create Mapper and Reducer for the Hadoop? 
1. org.apache.hadoop.mapreduce.Mapper 2. org.apache.hadoop.mapreduce.Reducer 
Qs. What Mapper does? 
Mapper is the first phase of Map phase which process map task. Mapper reads key/value pairs and emit key/value pair. Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs.
 
Qs. How many daemon processes run on a Hadoop cluster? 
Hadoop is comprised of five separate daemons. Each of these daemons runs in its own JVM. Following 3 Daemons run on Master nodes. NameNode ¬ This daemon stores and maintains the metadata for HDFS.The namenode is the master server in Hadoop and manages the file system namespace and access to the files stored in the cluster. Secondary NameNode ¬Secondary namenode, isn't a redundant daemon for the namenode but instead provides period checkpointing and housekeeping taskse. JobTracker ¬ Each cluster will have a single jobtracker that manages MapReduce jobs, distributes individual tasks to machines running the Task Tracker. Following 2 Daemons run on each Slave nodes DataNode – Stores actual HDFS data blocks.The datanode manages the storage attached to a node, of which there can be multiple nodes in a cluster. Each node storing data will have a datanode daemon running. TaskTracker – It is Responsible for instantiating and monitoring individual Map and Reduce tasks i.e.Tasktracker per datanode performs the actual work 

Qs. What is InputSplit in Hadoop? 
Input Split: It is part of input processed by a single map. Each split is processed by a single map. In other words InputSplit represents the data to be processed by an individual Mapper. Each split is divided into records , and the map processes each record, which is a key value pair. Split is basically a number of rows and record is that number. The length of the InputSplit is measured in bytes. Every InputSplit has a storage locations (hostname strings). The storage locations are used by the MapReduce system to place map tasks as close to split's data as possible. For example if data is 128 MB and block size if 64 MB(default) Case 1 ¬ Input split size [64 MB] = Block size [64 MB], # of Map task 2 Case 2 ¬ Input split size [32 MB] = Block size [64 MB], # of Map task 4 Case 2 ¬ Input split size [128 MB] = Block size [64 MB], # of Map task 1 
Qs. What is the InputFormat? 
The InputFormat class is one of the fundamental classes in the Hadoop Map Reduce framework. This class is responsible for defining two main things: Data splits Record reader 
1. Data split is a fundamental concept in Hadoop Map Reduce framework which defines both the size of individual Map tasks and its potential execution server. 
2. The Record Reader is responsible for actual reading records from the input file and submitting them (as key/value pairs) to the mapper. 
Qs. Consider case scenario: In M/R system, ¬ HDFS block size is 64 MB. Now Input format is FileInputFormat and we have 3 files of size 64K, 65Mb and 127Mb. How many input splits will be made by Hadoop framework? 
Hadoop will make 5 splits as follows: 1 split for 64K files 2 splits for 65MB files 2 splits for 127MB files 

Qs. What is JobTracker? 
JobTracker is the service within Hadoop that runs MapReduce jobs on the cluster. 

Qs. What if job tracker machine is down? 
In Hadoop 1.0, Job Tracker is single Point of availability means if JobTracker fails, all jobs must restart.Overall Execution flow will be interupted. Due to this limitation, In hadoop 2.0 Job Tracker concept is replaced by YARN. In YARN, the term JobTracker and TaskTracker has totally disappeared. YARN splits the two major functionalities of the JobTracker i.e. resource management and job scheduling/monitoring into 2 separate daemons (components). Resource Manager Node Manager(node specific)
 
Qs. What happens when a datanode fails ? 
When a datanode fails: Jobtracker and namenode detect the failure On the failed node all tasks are re¬scheduled Namenode replicates the users data to another node.
 
Qs. What are some typical functions of Job Tracker? 
The following are some typical tasks of JobTracker:¬ When Client applications submit map reduce jobs to the Job tracker. The JobTracker talks to the Name node to determine the location of the data. The JobTracker locates Tasktracker nodes with available slots at or near the data The JobTracker submits the work to the chosen Tasktracker nodes. The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker. When the work is completed, the JobTracker updates its status. Client applications can poll the JobTracker for information. 
Qs. Mention what are the main configuration parameters that user need to specify to run MapReduce Job?
 The user of Mapreduce framework needs to specify Job’s input locations in the distributed file system Job’s output location in the distributed file system Input format Output format Class containing the map function Class containing the reduce function JAR file containing the mapper, reducer and driver classes.

Qs. What is TaskTracker? 
TaskTracker is a node in the cluster that accepts tasks like MapReduce and Shuffle operations – from a JobTracker. 
1. Each Task Tracker is responsible to execute and manage the individual tasks assigned by Job Tracker 
2. Task Tracker also handles the data motion between the map and reduce phases. 
3. One Prime responsibility of Task Tracker is to constantly communicate with the Job Tracker the status of the Task. 
4. If the JobTracker fails to receive a heartbeat from a TaskTracker within a specified amount of time, it will assume the TaskTracker has crashed and will resubmit the corresponding tasks to other nodes in the cluster.
 
Qs.Explain what is heartbeat in HDFS? 
Heartbeat is referred to a signal used between a data node and Name node, and between task tracker and job tracker, if the Name node or job tracker does not respond to the signal, then it is considered there is some issues with data node or task tracker. 



Qs. Explain what is sqoop in Hadoop? 
Sqoop is a connectivity tool which transfers data in both directions between relational databases (MySQL, Oracle, Teradata) ,data warehouses and Hadoop HDFS and other Hadoop data source like Hive, HBase. Sqoop allows easy import and export of data from structured data stores. Sqoop integrates with Oozie, allowing you to schedule and automate import and export tasks. Example: A simple use case will be an organization that runs a nightly sqoop import to load the day's data from a production DB into a Hive data ware house for analysis. 

Qs. Suppose Hadoop spawned 100 tasks for a job and one of the task failed. What will Hadoop do?
 It will restart the task again on some other TaskTracker and only if the task fails more than four (default setting and can be changed) times will it kill the job. 

Qs. What is speculative execution (also called backup tasks)? What problem does it solve? 
In Hadoop during Speculative Execution a certain number of duplicate tasks are launched. On different slave node, multiple copies of same map or reduce task can be executed using Speculative Execution. In simple words, if a particular drive is taking long time to complete a task, Hadoop will create a duplicate task on another disk. Disk that finish the task first are retained and disks that do not finish first are killed. 

Qs. Explain the use of TaskTracker in the Hadoop cluster? 
1. A TaskTracker is a slave node in the cluster which that accepts the tasks from JobTracker like Map, Reduce or shuffle operation. TaskTracker also runs in its own JVM Process. 
2. Every TaskTracker is configured with a set of slots; these indicate the number of tasks that it can accept. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. 
3. The TaskTracker monitors these task instances, capturing the output and exit codes. When the Task instances finish, successfully or not, the task tracker notifies the JobTracker. 
4. The TaskTracker also send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These messages also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. 

Qs. Explain what are the basic parameters of a Mapper? 
The basic parameters of a Mapper are 1. LongWritable and Text 2. Text and IntWritable 

Qs. What is Distributed Cache in Hadoop? 
Distributed Cache is a facility provided by the MapReduce framework to cache files (text, archives, jars and so on) needed by applications during execution of the job. The framework will copy the necessary files to the slave node before any tasks for the job are executed on that node. 

Qs. What is the benefit of Distributed cache? 
Why can we just have the file in HDFS and have the application read it? Distributed cache is much faster. It copies the file to all trackers at the start of the job. Now if the task tracker runs 10 or 100 Mappers or Reducer, it will use the same copy of distributed cache. On the other hand, if you put code in file to read it from HDFS in the MR Job then every Mapper will try to access it from HDFS hence if a TaskTracker run 100 map jobs then it will try to read this file 100 times from HDFS. Also HDFS is not very efficient when used like this. 
Qs. How can you set an arbitrary number of Reducers to be created for a job in Hadoop? 
You can either do it programmatically by using method setNumReduceTasks in the Jobconf Class or set it up as a configuration setting. 

Qs. How will you write a custom partitioner for a Hadoop job? 
To have Hadoop use a custom partitioner you will have to do minimum the following three: Create a new class that extends Partitioner Class Override method getPartition In the wrapper that runs the Mapreduce, either Add the custom partitioner to the job programmatically using method set Partitioner Class or – add the custom partitioner to the job as a config file (if your wrapper reads from config file or oozie) 

Qs. How HDFA differs with NAS? 
Following are differences between HDFS and NAS In HDFS Data Blocks are distributed across local drives of all machines in a cluster, whereas in NAS data is stored on dedicated hardware. HDFS is designed to work with MapReduce System, since computation is moved to data. NAS is not suitable for MapReduce since data is stored separately from the computations. HDFS runs on a cluster of machines and provides redundancy using replication protocol. Whereas NAS is provided by a single machine therefore does not provide data redundancy.

Qs.  What is the meaning of speculative execution in Hadoop? Why is it important? 
Short Ans:-if task-tracker is not sending heartbeats to job-tracker then that particular task will be re-scheduled to another machine is called speculative execution. by default, speculative execution in hadoop can be disabled. 
Ans:Hadoop uses this speculative execution to mitigate the slow-task problem. Hadoop does not wait for a task to get failed. Whenever it is seen that a task is running slow, the Hadoop platform will schedule redundant copies of that task across several nodes which do not have other work to perform. The parallel tasks are monitored. As soon as one of the parallel tasks finishes successfully, Hadoop will use its output and kill the other parallel tasks. This process is termed as speculative execution. For enabling and disabling speculative execution of map and reduce tasks, the configuration parameters(Boolean) are mapred.map.tasks.speculative.execution and mapred.reduce.tasks.speculative.execution. By default both are set to true. 

Qs.  Difference between hadoop fs -put and hadoop fs -copyFromLocal? 
Ans1:Both are same intention just copy data from local system to HDFS, most often every Hadoop developer should aware of it. 
Put: Usage: hdfs dfs -put ... Copy single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and writes to destination file system. 
hdfs dfs -put localfile /user/hadoop/hadoopfile 
hdfs dfs -put localfile1 localfile2 /user/hadoop/hadoopdir 
hdfs dfs -put localfile hdfs://nn.example.com/hadoop/hadoopfile 
hdfs dfs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin. 
CopyFromLocal: Usage: hdfs dfs -copyFromLocal URI Similar to put command, except that the source is restricted to a local file reference. 
hdfs dfs -copyFromLocal '/home/venu/Desktop/somethingfile.xml' hdfs://nn.example.com/hadoop/ 

Qs. Hadoop input split size vs block size? 
Block is the physical representation of data. Split is the logical representation of data present in Block. Block and split size can be changed in properties. Map reads data from Block through splits i.e. split act as a broker between Block and Mapper. 
Consider two blocks: Block 1 aa bb cc dd ee ff gg hh ii jj Block 2 ww ee yy uu oo ii oo pp kk ll nn Now map reads block 1 till aa to JJ and doesn't know how to read block 2 i.e. block doesn't know how to process different block of information. Here comes a Split it will form a Logical grouping of Block 1 and Block 2 as single Block, then it forms offset(key) and line (value) using inputformat and record reader and send map to process further processing. If your resource is limited and you want to limit the number of maps you can increase the split size. For example: If we have 640 MB of 10 blocks i.e. each block of 64 MB and resource is limited then you can mention Split size as 128 MB then then logical grouping of 128 MB is formed and only 5 maps will be executed with a size of 128 MB. If we specify split size is false then whole file will form one input split and processed by one map which it takes more time to process when file is big. 
Two ways we can set split size
1) The parameter "mapred.max.split.size" which can be set per job 
2) By writing this code into java file 
final static long DEFAULT_SPLIT_SIZE = 128 * 1024 * 1024;
configuration.setLong(FileInputFormat.SPLIT_MAXSIZE, configuration.getLong(FileInputFormat.SPLIT_MAXSIZE,DEFAULT_SPLIT_SIZE ));

Qs.  How to find file biggest size in Unix /Hadoop?
1) -> du -Sh | sort -rh | head -n 3 returns file size in human readable format in descending order 
2) -> find . -type f -exec du -Sh {} + | sort -rh | head -n 5 
3) -> Finds directory find . -type f -exec du -Sh {} + | sort -rh | head -n 5 On Hadoop This commands returns files with size. hadoop fs -ls -h -R /data/pig/input -h is to display output in human readable format.

Qs.  What is namenode? 
The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. 
Qs. HADOOP - How NameNode Handles data node failures? 
-NameNode periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not recieved a hearbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replicated the system begins replicating the blocks that were stored on the dead datanode. The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode. 

Qs. HADOOP - Where is the Mapper Output (intermediate kay-value data) stored ? 
Map tasks write their output to the local disk, not to HDFS. Why is this? Map output is intermediate output: it's processed by reduce tasks to produce the final output, and once the job is complete the map output can be thrown away. So storing it in HDFS, with replication, would be overkill. 

Qs. What is the purpose of RecordReader in Hadoop?
The InputSplit has defined a slice of work, but does not describe how to access it. The RecordReader class actually loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper. The RecordReader instance is defined by the Input Format.

Qs. After the Map phase finishes, the Hadoop framework does “Partitioning, Shuffle and sort”. Explain what happens in this phase?
Partitioning: It is the process of determining which reducer instance will receive which intermediate keys and values. Each mapper must determine for all of its output (key, value) pairs which reducer will receive them. It is necessary that for any key, regardless1 of which mapper instance generated it, the destination partition is the same.
Shuffle: After the first map tasks have completed, the nodes may still be performing several more map tasks each. But they also begin exchanging the intermediate outputs from the map tasks to where they are required by the reducers. This process of moving map outputs to the reducers is known as shuffling.
Sort: Each reduce task is responsible for reducing the values associated with several intermediate keys. The set of intermediate keys on a single node is automatically sorted by Hadoop before they are presented to the Reducer.

Qs. What are some typical functions of Job Tracker?
 The following are some typical tasks of JobTracker:-
- Accepts jobs from clients
- It talks to the NameNode to determine the location of the data.
- It locates TaskTracker nodes with available slots at or near the data.
- It submits the work to the chosen TaskTracker nodes and monitors progress of each task by receiving heartbeat signals from Task tracker.

Qs. What mechanism does Hadoop framework provide to synchronise changes made in Distribution Cache during runtime of the application?
This is a tricky question. There is no such mechanism. Distributed Cache by design is read only during the time of Job execution.
Qs. Have you ever used Counters in Hadoop. Give us an example scenario?
Anybody who claims to have worked on a Hadoop project is expected to use counters.

Qs. How did you debug your Hadoop code?  
 There can be several ways of doing this but most common ways are:-
- By using counters.
- The web interface provided by Hadoop framework.

Qs. Did you ever built a production process in Hadoop? If yes, what was the process when your Hadoop job fails due to any reason?
It is an open-ended question but most candidates if they have written a production job, should talk about some type of alert mechanism like email is sent or there monitoring system sends an alert. Since Hadoop works on unstructured data, it is very important to have a good alerting system for errors since unexpected data can very easily break the job.

 Qs. What is a Combiner?
The Combiner is a ‘mini-reduce’ process which operates only on data generated by a mapper. The Combiner will receive as input all data emitted by the Mapper instances on a given node. The output from the Combiner is then sent to the Reducers, instead of the output from the Mappers.

Qs. What is the difference between TextInputFormat and KeyValueInputFormat class?
TextInputFormat: It reads lines of text files and provides the offset of the line as key to the Mapper and actual line as Value to the mapper.
KeyValueInputFormat: Reads text file and parses lines into key, Val1 pairs. Everything up to the first tab character is sent as key to the Mapper and the remainder of the line is sent as value to the mapper.

--New

Qs. Put file with specific block size *** The block size always in multiple of 512 bytes or 1024 etc....	
hadoop fs -Ddfs.block.size=3145728  -put src  dest
